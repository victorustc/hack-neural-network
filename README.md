这是对GitHub上https://github.com/Kayzaks/HackingNeuralNetworks 工作的一个小结，对其包括对里面攻击的介绍以及额外的延伸。但这里我以另外一种方式对机器学习中存在的安全威胁进行分类。

按照机器学习的过程对存在的攻击进行分类。

```
|——  training phase : 训练阶段的攻击

|—————|—————训练数据: 针对训练数据的攻击

|—————|—————|—————poisoning attack：往训练集中注入数据

|—————|—————|—————inference attack: 推测训练集数据的相关信息

|——  test phase : 测试阶段的攻击

|—————|—————测试数据: 针对测试数据的攻击

|—————|—————|—————adversarial examples：对抗性样本

|—————|—————模型 针对模型进行攻击

|—————|—————|—————模型权重：修改模型权重
```

